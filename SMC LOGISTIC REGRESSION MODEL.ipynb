{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bfddce5",
   "metadata": {},
   "source": [
    "# LOGISTIC REGRESSION MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2583027f-41aa-46f0-b7b0-5adb71cc08fb",
   "metadata": {
    "heading_collapsed": true,
    "tags": [],
    "toc-hr-collapsed": true
   },
   "source": [
    "## DATA PREPROCESSING."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a923a5a1-194d-4b63-8b8d-f371211a099c",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "###  install packages on the python environment, if not yet present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "547c1d6f-c11b-45b7-bac1-e1552733cfce",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# %pip install -I jinja2==3.0.3 --user\n",
    "# %pip install scikit-learn==1.1.3\n",
    "# %pip install statsmodels\n",
    "# %pip install particles\n",
    "# %pip install pandas\n",
    "# %pip install matplotlib\n",
    "# %conda install -c \"conda-forge/label/broken\" nbconvert-webpdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "%matplotlib inline\n",
    "import particles\n",
    "from particles import distributions as dists\n",
    "from particles import resampling as rs\n",
    "from particles import smc_samplers as ssps\n",
    "from particles.collectors import Moments\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b5a4c9-efcd-4b86-a726-87db36b0c70e",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "### ETS steps on KDD99 set with feature reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64021040",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### Import,variance treshold, multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f9b98029-ae96-46a2-b53f-8a618a389977",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs and features : (25192, 42)\n",
      "obs full dum (25192, 115)\n",
      "amount of features in the training dataset after variance reductions:  21\n",
      "amount of features in the training dataset after multicollinearity reductions:  7\n",
      "amount of observations in the training dataset before subsetting:  25192\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['count',\n",
       " 'srv_count',\n",
       " 'duration',\n",
       " 'hot',\n",
       " 'num_file_creations',\n",
       " 'dst_bytes',\n",
       " 'src_bytes']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import the dataset as a pandas dataframe\n",
    "train_data_url = \"https://raw.githubusercontent.com/stijnhuysman/DissertationData/main/Train_data.csv\"\n",
    "dftot = pd.read_csv(train_data_url)\n",
    "\n",
    "print(\"obs and features :\",dftot.shape)\n",
    "####################\n",
    "# FEATURES DATASET #\n",
    "####################\n",
    "# creating dummy variables on the categorical values\n",
    "Xtrain = pd.get_dummies(data=dftot.iloc[:,:(-1)] ,  \n",
    "                        drop_first=True,\n",
    "                        columns=['protocol_type', 'service', 'flag']\n",
    "                         , dtype=np.uint0)\n",
    "print(\"obs full dum\", Xtrain.shape)\n",
    "def variance_threshold_selector(data, threshold=0.5):\n",
    "    selector = VarianceThreshold(threshold*(1-threshold))\n",
    "    selector.fit_transform(data)\n",
    "    return data[data.columns[selector.get_support(indices=True)]]\n",
    "\n",
    "#reduced dataframe where 80 %  variance is explained\n",
    "Xtrain_selected = variance_threshold_selector(Xtrain, 0.8)\n",
    "print(\"amount of features in the training dataset after variance reductions: \" ,len(Xtrain_selected.columns))\n",
    "# Xtrain_selected.head()\n",
    "\n",
    "\n",
    "####################\n",
    "# RESPONSE VARIABLES #\n",
    "####################\n",
    "\n",
    "Y = dftot.loc[:,'class']\n",
    "Y = pd.get_dummies(data=Y, drop_first = True)\n",
    "\n",
    "\n",
    "####################\n",
    "# MERGING DATA      #\n",
    "####################\n",
    "\n",
    "Intrusion_full = pd.merge(Xtrain_selected, Y, left_index = True, right_index=True)\n",
    "# Intrusion_full.count()\n",
    "\n",
    "####################\n",
    "# MULTICOLLINEARITY REDUCTION      #\n",
    "####################\n",
    "# Use variance inflation factor to identify any significant multi-collinearity\n",
    "# !!!!!! ONLY THE FEATURES WITH VIF less or equal to 5 are kept !!!!!\n",
    "# 5 as setpoint #\n",
    "\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "def calc_vif(df):\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = df.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(df.shape[1])]\n",
    "    return(vif)\n",
    "\n",
    "VIF_table = calc_vif(Xtrain_selected).sort_values(by=['VIF'], ascending=False)\n",
    "VIF_select = VIF_table[(VIF_table['VIF'] <= 5)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "selection = VIF_select.iloc[:,0].values.tolist()[-5:]\n",
    "selection = VIF_select.iloc[:,0].values.tolist()\n",
    "X_selected = Xtrain_selected[selection]\n",
    "\n",
    "print(\"amount of features in the training dataset after multicollinearity reductions: \" ,len(X_selected.columns))\n",
    "print(\"amount of observations in the training dataset before subsetting: \" ,len(X_selected.index))\n",
    "Intrusion = pd.merge(X_selected, Y, left_index = True, right_index=True)\n",
    "Intrusion.head()\n",
    "# column_names = keeping a list of the columnames for later use\n",
    "column_names = Intrusion.columns.tolist()[:-1]  \n",
    "\n",
    "column_names\n",
    "Intrusion.head()\n",
    "# print(selection, column_names)\n",
    "column_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db035863-d1ac-4aa8-b8e2-88b4e49c8274",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "#### Standardize selected features for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1197f00d-28f9-46fc-8d0b-c8ec4ac88363",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has  25192  observations and  7  features.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "25192"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "####################\n",
    "# STANDARDIZE DATA FOR LOGISTIC REGRESSION      #\n",
    "####################\n",
    "# here following the procedure as explained by Chopin : outcome values as -1 and 1\n",
    "# also the features are normalized to each have their normal(0,1) distribution\n",
    "# finally, from here, all data is handled as arrays, for speeding up calculations\n",
    "\n",
    "def prepare_predictors(predictors, add_intercept=True, scale=0.5):\n",
    "    preds = np.atleast_2d(predictors)  # in case predictors is (n,)\n",
    "    rescaled_preds = scale * (preds - np.mean(preds, axis=0)) / np.std(preds, axis=0)\n",
    "    if add_intercept:\n",
    "        n, p = preds.shape\n",
    "        out = np.empty((n, p + 1))\n",
    "        out[:, 0] = 1. # intercept\n",
    "        out[:, 1:] = rescaled_preds\n",
    "    else:\n",
    "        out = rescaled_preds\n",
    "    return out\n",
    "\n",
    "   \n",
    "    \n",
    "def preprocess(raw_data, return_y=False):\n",
    "    response = 2 * raw_data[:, -1] - 1  # 0/1 -> -1/1\n",
    "    preds = prepare_predictors(raw_data[:, :-1])\n",
    "    if return_y:\n",
    "        return preds, response\n",
    "    else:\n",
    "        return preds * response[:, np.newaxis]\n",
    "    \n",
    "    \n",
    "raw_data = Intrusion.to_numpy()\n",
    "data = preprocess(raw_data)\n",
    "\n",
    "T, p = data.shape  \n",
    "print(\"The data has \", T, \" observations and \",p-1,\" features.\" )\n",
    "data[:10]\n",
    "T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974a06a1-6977-4aa2-8563-bc25961fb03c",
   "metadata": {
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Defining outcome array  Y and Selected features X, including a Unit (1) column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "600db470-881e-4ec9-bc1b-e280daf4b627",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Y = data[:, [0]]\n",
    "# # X = data[:, [1,8]]\n",
    "# # X = np.delete(data, 0, axis=1)\n",
    "# X = data\n",
    "# X[:,0] = np.ones(T)  \n",
    "# X[:3]   #for the X calc, the first column is replaced by 1, \n",
    "# # in a later step, this will be used for matrix calculation of the intercept\n",
    "# Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a4c832-60ab-4b48-8339-9f0294a929b4",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "#### Subsetting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bead9a12-e6bd-4d61-b7de-a11793151119",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has  25192  observations and  7  features.\n"
     ]
    }
   ],
   "source": [
    "#SUBSET DATA TO nobs  OBSERVATIONS in line\n",
    "import numpy as np\n",
    "observations = T\n",
    "nobs = T\n",
    "# nobs = 15000  #HERE CHOOSING A SPECIFIC AMOUNT\n",
    "startpos = int(np.random.randint(low = 0, high = observations - nobs+1, size=1,dtype=np.int64))\n",
    "# startpos = 0\n",
    "endpos = startpos + nobs\n",
    "# Intrusion = Intrusion_full.iloc[startpos:endpos]\n",
    "# print('Subsetted the data to ', len(Intrusion.index), ' sequential observations.')\n",
    "\n",
    "\n",
    "# the dataset data is a list of arrays\n",
    "dataset = data[startpos:endpos]\n",
    "len(dataset)\n",
    "T, p = dataset.shape  \n",
    "print(\"The data has \", T, \" observations and \",p-1,\" features.\" )\n",
    "# print(dataset)\n",
    "\n",
    "Y =  [int(elem[0]) for elem in dataset]\n",
    "# print(Y)\n",
    "\n",
    "X = dataset.copy()\n",
    "X[:,0] = np.ones(T)  \n",
    "#for the X calc, the first column is replaced by 1, \n",
    "# in a later step, this will be used for matrix calculation of the intercept\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62194c16-76aa-42ac-878d-1f6b01d7a0c7",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## LOGISTIC REGRESSION MODEL  and FK objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06855003-7838-46cf-88f0-07bee93938c6",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "### Setting parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "45f8d0ff-f482-4920-848c-46df6de38ac5",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount of betas:  8\n",
      "different amount of particles sampled:  [10, 100, 1000]\n"
     ]
    }
   ],
   "source": [
    "######ENTER_PARAMETER#######\n",
    "alg_type = 'ibis'\n",
    "N_part = [10, 100, 1000]                          #  N0  = 200 000 => number of resampled particles\n",
    "p  #amount of betas\n",
    "\n",
    "print(\"amount of betas: \", p)\n",
    "print(\"different amount of particles sampled: \" ,N_part) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb13f99-b72b-4db8-9684-aed706e5be2e",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "### Defining the Bayesian model for logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e407be37-6b89-4b28-847d-a3630a4c8e3f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('beta', 'float64', 8)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PRIOR AND MODEL\n",
    "### DEFINITION OF THE PRIOR PARAMETERS\n",
    "######ENTER_PARAMETER#######\n",
    "\n",
    "scales = 0.8 * np.ones(p)     # for all column beta scales  of 5 = >variance\n",
    "scales[0] = 20  # intercept has a larger scale   #the beta_0 of course is larger, we adjust it to start from\n",
    "\n",
    "### AMOUNT OF RUNS OF THE SAMPLER\n",
    "nruns = 1\n",
    "\n",
    "###empty result list to track the summaries\n",
    "results = []\n",
    "\n",
    "\n",
    "######################################################\n",
    "# #################PRIOR###########################\n",
    "######################################################\n",
    "\n",
    "'''Define a prior with multivariate Normal distro'''\n",
    "\n",
    "prior = dists.StructDist({'beta':dists.MvNormal(scale=scales,\n",
    "                                                cov=np.eye(p))})\n",
    "\n",
    "#the prior is nothing more than a MVN distribution with an eye cov matrix\n",
    "\n",
    "######################################################\n",
    "# ###BAYESIAN MODEL : LIKELIHOOD X PRIOR #############\n",
    "######################################################\n",
    "\n",
    "class LogisticRegression(ssps.StaticModel):  #build  a SMC sampler LR class\n",
    "    def logpyt(self, theta, t):\n",
    "        # log-likelihood factor t, for given theta\n",
    "        lin = np.matmul(theta['beta'], data[t, :])\n",
    "        #Matrix product of two arrays.  => the logmodil takes all X-es and multiplies with the beta matrix (that is a multivariate normal of dimention\n",
    "        \n",
    "        \n",
    "        return - np.logaddexp(0., -lin)\n",
    "\n",
    "    #  the numpy.logaddexp() function is used to calculate Logarithm of the sum of exponentiations of the inputs.\n",
    "    #  This function is useful in statistics where the calculated probabilities of events \n",
    "    # may be so small as to exceed the range of normal floating point numbers.  In such cases\n",
    "    # the logarithm of the calculated probability is stored. This function\n",
    "    # allows adding probabilities stored in such a fashion.\n",
    "    \n",
    "    \n",
    "# nruns = 100\n",
    "# nsteps = 100\n",
    "prior.dtype  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9b747b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "'''\n",
    "now we model the Logistic Regression as a classifier and build a sampler for it\n",
    "\n",
    "interesting info :  see also : https://towardsdatascience.com/bayesian-logistic-regression-53df017ba90f\n",
    "\n",
    "The starting point for Bayesian Logistic Regression is Bayes’ Theorem, \n",
    "which formally states that the posterior distribution of parameters \n",
    "is proportional to the product of two quantities: \n",
    "the likelihood of observing the data given the parameters and the prior density of parameters. \n",
    "Applied to our context this can intuitively be understood as follows: \n",
    "\n",
    "our posterior beliefs around the logistic regression coefficients are formed by \n",
    "both our prior beliefs and the evidence we observe (i.e. the data).\n",
    "\n",
    "Under the assumption that individual label-feature pairs are independently and identically distributed,\n",
    "their joint likelihood is simply the product over their individual densities (Bernoulli).\n",
    "\n",
    "Solving the problem\n",
    "In practice we do not maximize the posterior directly. \n",
    "Instead we minimize the negative log likelihood, which is equivalent and easier to implement.\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09276f4d-a56a-40c8-9149-6a8474eb323b",
   "metadata": {
    "hidden": true,
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Create a FK object based on the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0588940c-a70f-4d6c-9ad9-28e51ec4a273",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "### Define the options to resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17763226-f1de-45a5-adc8-d0f9404bc53e",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#################################\"\"\"\n",
    "#RESAMPLING METHODS\n",
    "\n",
    "# print('Dataset: %s' % dataset_name)\n",
    "# resampling = ['multinomial', 'residual', 'stratified', 'systematic', 'ssp']\n",
    "# for rsmethod in resampling:\n",
    "    # print(rsmethod)\n",
    "\n",
    "    \n",
    "######ENTER_PARAMETER#######\n",
    "resampling = ['systematic', 'ssp','stratified']\n",
    "# resampling = 'systematic'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829c2b09-1ea9-4231-bc6f-b8b0dd4eb168",
   "metadata": {
    "tags": []
   },
   "source": [
    "## RUN SAMPLERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f188a71-a9b4-4e89-b643-edf40af42b48",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Run multiple particle filters, using multiple processors together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91549694-a571-4013-884a-7cfecc34f43a",
   "metadata": {},
   "source": [
    "  !!!***CAREFUL THE CELL CAN TAKE A LOT OF TIME BASED ON COMBINATIONS AND PARAMS*** !!!\n",
    "  current params :\n",
    "  The time of execution of above program with  3  combinatons took : 780.7906959056854 s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05ad37",
   "metadata": {},
   "source": [
    "PARAMETER BOX\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55f9f660",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_part = list(range(10,501,20))\n",
    "len(N_part)\n",
    "chains = list(range(10, 101, 20))\n",
    "len(chains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4b47ecdc-06b9-4780-aef8-bd989d9fa25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nruns =1\n",
    "# N_part = [10, 1000, 5000]\n",
    "# particles.multiSMC?\n",
    "scales = 0.5 * np.ones(p)     \n",
    "scales[0] = 20  \n",
    "prior = dists.StructDist({'beta':dists.MvNormal(scale=scales, cov=np.eye(p))})\n",
    "model = LogisticRegression(data=dataset[-5000:], prior=prior) #the model is the base for the FK element \n",
    "fullmodel = LogisticRegression(data=dataset[:], prior=prior) #the model is the base for the FK element \n",
    "\n",
    "fk10 = ssps.IBIS(model=model, len_chain=10)\n",
    "fk20 = ssps.IBIS(model=model, len_chain=20)\n",
    "fk30 = ssps.IBIS(model=model, len_chain=30)\n",
    "\n",
    "fk_objects = [fk10, fk20, fk30]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e9b66705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import particles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc912322",
   "metadata": {},
   "source": [
    "## THIS SAMPLER BELOW CALCULATES ALL THE POSSIBILITIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b9ca0cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time of execution of above program with  1  combinatons and  100  particles and chain:  10 took : 89.87825179100037 s\n",
      "The time of execution of above program with  1  combinatons and  300  particles and chain:  10 took : 179.435485124588 s\n",
      "The time of execution of above program with  1  combinatons and  100  particles and chain:  50 took : 392.65454864501953 s\n",
      "The time of execution of above program with  1  combinatons and  300  particles and chain:  50 took : 724.4963138103485 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[89.87825179100037, 179.435485124588, 392.65454864501953, 724.4963138103485]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import time module\n",
    "import time\n",
    "import particles\n",
    "from particles import smc_samplers as ssps\n",
    "\n",
    "duration_part = []\n",
    "MASTER = []\n",
    "# for c in chains:\n",
    "for c in [10, 50]:\n",
    "    fk = ssps.IBIS(model=fullmodel, len_chain=c)\n",
    "    for n in [100,300]:\n",
    "        start = time.time()\n",
    "        resultaat = particles.multiSMC(nruns=nruns,     #A sampling runs\n",
    "                                       nprocs=0,\n",
    "                                       fk=fk,   #B FK objects\n",
    "                                       N=n,           #C N options \n",
    "                                       collect=[Moments], \n",
    "                                       verbose=False,\n",
    "                                       resampling = 'systematic')  #D  resampling options\n",
    "        # resultaat = particles.multiSMC(nruns=1, fk=fk, N=50, collect=[Moments], verbose=False, resampling = 'systematic')\n",
    "\n",
    "        #AANTAL COMBINATIES = A*B*C*D\n",
    "        # combinations = nruns*len(fk_objects)*len(N_part)*1\n",
    "        combinations = len(resultaat)    \n",
    "        MASTER.append(resultaat)\n",
    "        # # endtime and print\n",
    "        end = time.time()\n",
    "        duration_part.append(end-start)\n",
    "        print(\"The time of execution of above program with \", combinations, \" combinatons and \", n, \" particles and chain: \", c, \"took :\",\n",
    "              (end-start) , \"s\")\n",
    "\n",
    "duration_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5cdf24c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAME CALCULATEION BUT MULTIPROCESSED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "727b52cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time of execution of above program with  4  combinations,  took : 744.6186916828156 s\n"
     ]
    }
   ],
   "source": [
    "# Import time module\n",
    "import time\n",
    "import particles\n",
    "from particles import smc_samplers as ssps\n",
    "\n",
    "fk = [ssps.IBIS(model=fullmodel, len_chain=10), ssps.IBIS(model=fullmodel, len_chain=50)]\n",
    "start = time.time()\n",
    "MASTER2 = particles.multiSMC(nruns=nruns,     #A sampling runs\n",
    "                               nprocs=0,\n",
    "                               fk=fk,   #B FK objects\n",
    "                               N=[100,300],           #C N options \n",
    "                               collect=[Moments], \n",
    "                               verbose=False,\n",
    "                               resampling = 'systematic')  #D  resampling options\n",
    "end = time.time()\n",
    "duration_part.append(end-start)\n",
    "print(\"The time of execution of above program with \", len(MASTER2), \" combinations, \" , \"took :\",\n",
    "      (end-start) , \"s\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cb59ca51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'run': 0,\n",
       "  'fk': <particles.smc_samplers.IBIS at 0x251ceb29700>,\n",
       "  'N': 100,\n",
       "  'seed': 434510515,\n",
       "  'output': <particles.core.SMC at 0x251ceb02880>},\n",
       " {'run': 0,\n",
       "  'fk': <particles.smc_samplers.IBIS at 0x251ceb29700>,\n",
       "  'N': 300,\n",
       "  'seed': 1389608321,\n",
       "  'output': <particles.core.SMC at 0x251ceb0ffd0>},\n",
       " {'run': 0,\n",
       "  'fk': <particles.smc_samplers.IBIS at 0x251ceb29310>,\n",
       "  'N': 100,\n",
       "  'seed': 2234645933,\n",
       "  'output': <particles.core.SMC at 0x2519138d430>},\n",
       " {'run': 0,\n",
       "  'fk': <particles.smc_samplers.IBIS at 0x251ceb29310>,\n",
       "  'N': 300,\n",
       "  'seed': 3541807894,\n",
       "  'output': <particles.core.SMC at 0x251ceb02c70>}]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MASTER2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "59502ee3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(100, 10, 84.307419),\n",
       " (300, 10, 175.5365938),\n",
       " (100, 50, 389.30648750000006),\n",
       " (300, 50, 720.9583277)]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chain, N, duration = null\n",
    "TRACKING = []\n",
    "for i in range(len(MASTER)):\n",
    "# for i in [0]:\n",
    "\n",
    "    res = MASTER[i][0]\n",
    "    \n",
    "    TRACKING.append((MASTER[i][0]['output'].N, MASTER[i][0]['output'].fk.len_chain, MASTER[i][0]['output'].cpu_time))\n",
    "    \n",
    "TRACKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bdbc0f0b-6f0d-4abc-a67e-66095fee2604",
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "8-th leading minor of the array is not positive definite",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\stijn.huysman\\Anaconda3\\envs\\StijnDissEnvironment1_20230106\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 428, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\stijn.huysman\\Anaconda3\\envs\\StijnDissEnvironment1_20230106\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 275, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\stijn.huysman\\Anaconda3\\envs\\StijnDissEnvironment1_20230106\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 620, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\stijn.huysman\\Anaconda3\\envs\\StijnDissEnvironment1_20230106\\lib\\site-packages\\joblib\\parallel.py\", line 288, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\stijn.huysman\\Anaconda3\\envs\\StijnDissEnvironment1_20230106\\lib\\site-packages\\joblib\\parallel.py\", line 288, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\stijn.huysman\\OneDrive - UGent\\git\\particles\\particles\\utils.py\", line 210, in __call__\n    return self.func(**kwargs)\n  File \"C:\\Users\\stijn.huysman\\OneDrive - UGent\\git\\particles\\particles\\core.py\", line 452, in __call__\n    pf.run()\n  File \"C:\\Users\\stijn.huysman\\OneDrive - UGent\\git\\particles\\particles\\utils.py\", line 88, in timed_method\n    out = method(self, **kwargs)\n  File \"C:\\Users\\stijn.huysman\\OneDrive - UGent\\git\\particles\\particles\\core.py\", line 438, in run\n    for _ in self:\n  File \"C:\\Users\\stijn.huysman\\OneDrive - UGent\\git\\particles\\particles\\core.py\", line 410, in __next__\n    self.resample_move()\n  File \"C:\\Users\\stijn.huysman\\OneDrive - UGent\\git\\particles\\particles\\core.py\", line 356, in resample_move\n    self.rs_flag = self.fk.time_to_resample(self)\n  File \"C:\\Users\\stijn.huysman\\OneDrive - UGent\\git\\particles\\particles\\smc_samplers.py\", line 741, in time_to_resample\n    self.move.calibrate(smc.W, smc.X)\n  File \"C:\\Users\\stijn.huysman\\OneDrive - UGent\\git\\particles\\particles\\smc_samplers.py\", line 642, in calibrate\n    self.mcmc.calibrate(W, x)\n  File \"C:\\Users\\stijn.huysman\\OneDrive - UGent\\git\\particles\\particles\\smc_samplers.py\", line 602, in calibrate\n    x.shared['chol_cov'] = scale * linalg.cholesky(cov, lower=True)\n  File \"C:\\Users\\stijn.huysman\\Anaconda3\\envs\\StijnDissEnvironment1_20230106\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 88, in cholesky\n    c, lower = _cholesky(a, lower=lower, overwrite_a=overwrite_a, clean=True,\n  File \"C:\\Users\\stijn.huysman\\Anaconda3\\envs\\StijnDissEnvironment1_20230106\\lib\\site-packages\\scipy\\linalg\\_decomp_cholesky.py\", line 37, in _cholesky\n    raise LinAlgError(\"%d-th leading minor of the array is not positive \"\nnumpy.linalg.LinAlgError: 8-th leading minor of the array is not positive definite\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m N_part:\n\u001b[0;32m     10\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 11\u001b[0m     resultaat \u001b[38;5;241m=\u001b[39m \u001b[43mparticles\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultiSMC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnruns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnruns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m     \u001b[49m\u001b[38;5;66;43;03m#A sampling runs\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mfk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m#B FK objects\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mN\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m           \u001b[49m\u001b[38;5;66;43;03m#C N options \u001b[39;49;00m\n\u001b[0;32m     15\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcollect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mMoments\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mresampling\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msystematic\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m#D  resampling options\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# resultaat = particles.multiSMC(nruns=1, fk=fk, N=50, collect=[Moments], verbose=False, resampling = 'systematic')\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m#AANTAL COMBINATIES = A*B*C*D\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# combinations = nruns*len(fk_objects)*len(N_part)*1\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     combinations \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(resultaat)    \n",
      "File \u001b[1;32m~\\OneDrive - UGent\\git\\particles\\particles\\core.py:541\u001b[0m, in \u001b[0;36mmultiSMC\u001b[1;34m(nruns, nprocs, out_func, collect, **args)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;124;03m\"\"\"Run SMC algorithms in parallel, for different combinations of parameters.\u001b[39;00m\n\u001b[0;32m    463\u001b[0m \n\u001b[0;32m    464\u001b[0m \u001b[38;5;124;03m`multiSMC` relies on the `multiplexer` utility, and obeys the same logic.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;124;03m`utils.multiplexer`: for more details on the syntax.\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    540\u001b[0m f \u001b[38;5;241m=\u001b[39m _identity \u001b[38;5;28;01mif\u001b[39;00m out_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m _picklable_f(out_func)\n\u001b[1;32m--> 541\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m utils\u001b[38;5;241m.\u001b[39mmultiplexer(f\u001b[38;5;241m=\u001b[39mf, nruns\u001b[38;5;241m=\u001b[39mnruns, nprocs\u001b[38;5;241m=\u001b[39mnprocs, seeding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m                          protected_args\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcollect\u001b[39m\u001b[38;5;124m'\u001b[39m: collect},\n\u001b[0;32m    543\u001b[0m                          \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32m~\\OneDrive - UGent\\git\\particles\\particles\\utils.py:267\u001b[0m, in \u001b[0;36mmultiplexer\u001b[1;34m(f, nruns, nprocs, seeding, protected_args, **args)\u001b[0m\n\u001b[0;32m    265\u001b[0m         op[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m seed\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# the actual work happens here\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdistribute_work\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnprocs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - UGent\\git\\particles\\particles\\utils.py:177\u001b[0m, in \u001b[0;36mdistribute_work\u001b[1;34m(f, inputs, outputs, nprocs, out_key)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;66;03m# multiprocessing\u001b[39;00m\n\u001b[0;32m    176\u001b[0m pool \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mParallel(n_jobs\u001b[38;5;241m=\u001b[39mnprocs, backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloky\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mip\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mip\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results):\n\u001b[0;32m    179\u001b[0m     add_to_dict(outputs[i], r)\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\StijnDissEnvironment1_20230106\\lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\StijnDissEnvironment1_20230106\\lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\StijnDissEnvironment1_20230106\\lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\StijnDissEnvironment1_20230106\\lib\\concurrent\\futures\\_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 446\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\StijnDissEnvironment1_20230106\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mLinAlgError\u001b[0m: 8-th leading minor of the array is not positive definite"
     ]
    }
   ],
   "source": [
    "# Import time module\n",
    "\n",
    "\n",
    "duration_part = []\n",
    "MASTER = []\n",
    "\n",
    "#DO NOT EXECUTE!!!! TAKES TIME\n",
    "for fk in fk_objects:\n",
    "    for n in N_part:\n",
    "        start = time.time()\n",
    "        resultaat = particles.multiSMC(nruns=nruns,     #A sampling runs\n",
    "                                       nprocs=0,\n",
    "                                       fk=fk,   #B FK objects\n",
    "                                       N=n,           #C N options \n",
    "                                       collect=[Moments], \n",
    "                                       verbose=False,\n",
    "                                       resampling = 'systematic')  #D  resampling options\n",
    "        # resultaat = particles.multiSMC(nruns=1, fk=fk, N=50, collect=[Moments], verbose=False, resampling = 'systematic')\n",
    "\n",
    "        #AANTAL COMBINATIES = A*B*C*D\n",
    "        # combinations = nruns*len(fk_objects)*len(N_part)*1\n",
    "        combinations = len(resultaat)    \n",
    "        MASTER.append(resultaat)\n",
    "        # # endtime and print\n",
    "        end = time.time()\n",
    "        duration_part.append(end-start)\n",
    "        print(\"The time of execution of above program with \", combinations, \" combinatons and \", n, \" particles took :\",\n",
    "              (end-start) , \"s\")\n",
    "\n",
    "duration_part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a825d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combinations: \",(MASTER[0][0]['output'].N, MASTER[1][0]['output'].fk.len_chain),\n",
    "     (MASTER[1][0]['output'].N, MASTER[1][0]['output'].fk.len_chain),\n",
    "     (MASTER[2][0]['output'].N, MASTER[1][0]['output'].fk.len_chain),\n",
    "     (MASTER[3][0]['output'].N, MASTER[1][0]['output'].fk.len_chain),)\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee062b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "MASTER[1][0]['output'].summaries.moments[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac2d72a8-5855-4040-9166-55e013705bf4",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e57d1c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# CALCULATE ALL POSSIBILITIES\n",
    "a = T   #if we use T : we use ALL the observations on scope\n",
    "\n",
    "a = 0# for i in range(len(resultaat)):   #for EVERY COMBINATION\n",
    "\n",
    "SUMMARY = []\n",
    "for i in range(len(MASTER2)):   #for EVERY COMBINATION\n",
    "    OUT = MASTER2[i]['output']\n",
    "    MOMENTS = OUT.summaries.moments[a:]\n",
    "    particles_used =OUT.N\n",
    "    chainlength = OUT.fk.len_chain\n",
    "    betameans_set = [entry['mean'].tolist()[0].tolist() for entry in MOMENTS]\n",
    "    betavar_set = [entry['var'].tolist()[0].tolist() for entry in MOMENTS ]\n",
    "    upper_set = [entry['mean'].tolist()[0].tolist() + 1.96*np.sqrt(entry['var'].tolist()[0].tolist()) for entry in MOMENTS]\n",
    "    lower_set = [entry['mean'].tolist()[0].tolist() - 1.96*np.sqrt(entry['var'].tolist()[0].tolist()) for entry in MOMENTS]\n",
    "    \n",
    "     # Listing the estimates per feature (beta, var, under , upper), ready to plot\n",
    "    SUMMARY.append((particles_used, chainlength, betameans_set, betavar_set, lower_set,upper_set) )\n",
    "    \n",
    "print(SUMMARY[0][0], SUMMARY[0][1])\n",
    "print(SUMMARY[1][0], SUMMARY[1][1])\n",
    "print(SUMMARY[2][0], SUMMARY[2][1])\n",
    "print(SUMMARY[3][0], SUMMARY[3][1])\n",
    "\n",
    "\n",
    "amountofbetas = len(SUMMARY[0][2][0])\n",
    "\n",
    "for i in range(len(SUMMARY)):\n",
    "    #############################################################\n",
    "    #NOW TAKE ALL VALUES FOR THE BETAS AND CONFIDENCE BANDS\n",
    "    #############################################################\n",
    "    \n",
    "    for j in range(amountofbetas):\n",
    "        globals()[f\"beta{i}_{j}\"] = [sublist[j] for sublist in SUMMARY[i][2]]\n",
    "        globals()[f\"var{i}_{j}\"] = [sublist[j] for sublist in SUMMARY[i][3]]\n",
    "        globals()[f\"under{i}_{j}\"] = [sublist[j] for sublist in SUMMARY[i][4]]\n",
    "        globals()[f\"upper{i}_{j}\"] = [sublist[j] for sublist in SUMMARY[i][5]]\n",
    "\n",
    "    # Listing a numbered list of observations\n",
    "    observations = list(range(len(beta0_0)))\n",
    "    particles_used \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a4ad45",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### PLOTTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68251d7",
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#############################################################\n",
    "#PLOT THEM \n",
    "#############################################################\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(30, 40))\n",
    "fig.suptitle(\n",
    "    str('Logistic regression')).set_fontsize(60)\n",
    "\n",
    "fig.subplots_adjust(bottom=.60)\n",
    "ft = 40\n",
    "\n",
    "linewidth = 6\n",
    "col = 'lightgrey'\n",
    "col = \"#EBECF0\"\n",
    "\n",
    "\n",
    "axs[0,0].plot(observations, beta0_0 , label='N: 100, chain 10 ', linewidth=linewidth)\n",
    "axs[0,0].plot(observations, beta1_0 , label='N: 300, chain 10 ', linewidth=linewidth)\n",
    "axs[0,0].plot(observations, beta2_0 , label='N: 100, chain 50 ', linewidth=linewidth)\n",
    "axs[0,0].plot(observations, beta3_0, label='N: 300, chain 50 ', linewidth=linewidth)\n",
    "axs[0,0].legend(fontsize=ft)\n",
    "axs[0,0].set_facecolor(col)\n",
    "\n",
    "\n",
    "\n",
    "# axs[0,0].fill_between(observations, under0_0, upper0_0, color='gray', alpha=0.5)\n",
    "# axs[0,0].fill_between(observations, under1_0, upper1_0, color='gray', alpha=0.5)\n",
    "# axs[0,0].fill_between(observations, under2_0, upper2_0, color='gray', alpha=0.5)\n",
    "# axs[0,0].fill_between(observations, under3_0, upper3_0, color='gray', alpha=0.5)\n",
    "\n",
    "axs[0,1].plot(observations, beta0_1, label='N: 100, chain 10 ', linewidth=linewidth)\n",
    "axs[0,1].plot(observations, beta1_1, label='N: 300, chain 10 ', linewidth=linewidth)\n",
    "axs[0,1].plot(observations, beta2_1, label='N: 100, chain 50 ', linewidth=linewidth)\n",
    "axs[0,1].plot(observations, beta3_1, label='N: 300, chain 50 ', linewidth=linewidth)\n",
    "axs[0,1].legend(fontsize=ft)\n",
    "axs[0,1].set_facecolor(col)\n",
    "\n",
    "\n",
    "\n",
    "axs[1,0].plot(observations, beta0_2, label='N: 100, chain 10 ', linewidth=linewidth)\n",
    "axs[1,0].plot(observations, beta1_2, label='N: 300, chain 10 ', linewidth=linewidth)\n",
    "axs[1,0].plot(observations, beta2_2, label='N: 100, chain 50 ', linewidth=linewidth)\n",
    "axs[1,0].plot(observations, beta3_2, label='N: 300, chain 50 ', linewidth=linewidth)\n",
    "axs[1,0].legend(fontsize=ft)\n",
    "axs[1,0].set_facecolor(col)\n",
    "\n",
    "axs[1,1].plot(observations, beta0_3, label='N: 100, chain 10 ', linewidth=linewidth)\n",
    "axs[1,1].plot(observations, beta1_3, label='N: 300, chain 10 ', linewidth=linewidth)\n",
    "axs[1,1].plot(observations, beta2_3, label='N: 100, chain 50 ', linewidth=linewidth)\n",
    "axs[1,1].plot(observations, beta3_3, label='N: 300, chain 50 ', linewidth=linewidth)\n",
    "axs[1,1].legend(fontsize=ft)\n",
    "axs[1,1].set_facecolor(col)\n",
    "\n",
    "axs[2,0].plot(observations, beta0_4, label='N: 100, chain 10 ', linewidth=linewidth)\n",
    "axs[2,0].plot(observations, beta1_4, label='N: 300, chain 10 ', linewidth=linewidth)\n",
    "axs[2,0].plot(observations, beta2_4, label='N: 100, chain 50 ', linewidth=linewidth)\n",
    "axs[2,0].plot(observations, beta3_4, label='N: 300, chain 50 ', linewidth=linewidth)\n",
    "axs[2,0].legend(fontsize=ft)\n",
    "axs[2,0].set_facecolor(col)\n",
    "\n",
    "axs[2,1].plot(observations, beta0_5, label='N: 100, chain 10 ', linewidth=linewidth)\n",
    "axs[2,1].plot(observations, beta1_5, label='N: 300, chain 10 ', linewidth=linewidth)\n",
    "axs[2,1].plot(observations, beta2_5, label='N: 100, chain 50 ', linewidth=linewidth)\n",
    "axs[2,1].plot(observations, beta3_5, label='N: 300, chain 50 ', linewidth=linewidth)\n",
    "axs[2,1].legend(fontsize=ft)\n",
    "axs[2,1].set_facecolor(col)\n",
    "\n",
    "axs[3,0].plot(observations, beta0_6, label='N: 100, chain 10 ', linewidth=linewidth)\n",
    "axs[3,0].plot(observations, beta1_6, label='N: 300, chain 10 ', linewidth=linewidth)\n",
    "axs[3,0].plot(observations, beta2_6, label='N: 100, chain 50 ', linewidth=linewidth)\n",
    "axs[3,0].plot(observations, beta3_6, label='N: 300, chain 50 ', linewidth=linewidth)\n",
    "axs[3,0].legend(fontsize=ft)\n",
    "axs[3,0].set_facecolor(col)\n",
    "\n",
    "axs[3,1].plot(observations, beta0_7, label='N: 100, chain 10 ', linewidth=linewidth)\n",
    "axs[3,1].plot(observations, beta1_7, label='N: 300, chain 10 ', linewidth=linewidth)\n",
    "axs[3,1].plot(observations, beta2_7, label='N: 100, chain 50 ', linewidth=linewidth)\n",
    "axs[3,1].plot(observations, beta3_7, label='N: 300, chain 50 ', linewidth=linewidth)\n",
    "axs[3,1].legend(fontsize=ft)\n",
    "axs[3,1].set_facecolor(col)\n",
    "\n",
    "\n",
    "m = .05\n",
    "\n",
    "\n",
    "under0 = np.min([beta0_0,beta1_0, beta2_0, beta3_0])\n",
    "upper0 = np.max([beta0_0,beta1_0, beta2_0, beta3_0])\n",
    "under1 = np.min([beta0_1,beta1_1, beta2_1, beta3_1])\n",
    "upper1 = np.max([beta0_1,beta1_1, beta2_1, beta3_1])\n",
    "under2 = np.min([beta0_2,beta1_2, beta2_2, beta3_2])\n",
    "upper2 = np.max([beta0_2,beta1_2, beta2_2, beta3_2])\n",
    "under3 = np.min([beta0_3,beta1_3, beta2_3, beta3_3])\n",
    "upper3 = np.max([beta0_3,beta1_3, beta2_3, beta3_3])\n",
    "under4 = np.min([beta0_4,beta1_4, beta2_4, beta3_4])\n",
    "upper4 = np.max([beta0_4,beta1_4, beta2_4, beta3_4])\n",
    "under5 = np.min([beta0_5,beta1_5, beta2_5, beta3_5])\n",
    "upper5 = np.max([beta0_5,beta1_5, beta2_5, beta3_5])\n",
    "under6 = np.min([beta0_6,beta1_6, beta2_6, beta3_6])\n",
    "upper6 = np.max([beta0_6,beta1_6, beta2_6, beta3_6])\n",
    "under7 = np.min([beta0_7,beta1_7, beta2_7, beta3_7])\n",
    "upper7 = np.max([beta0_7,beta1_7, beta2_7, beta3_7])\n",
    "\n",
    "#     #set the y limits manually\n",
    "axs[0,0].set_ylim([np.min(under0)-np.abs(np.min(under0))*m,np.max(upper0 + np.abs(np.max(upper0))*m)])\n",
    "axs[0,1].set_ylim([np.min(under1)-np.abs(np.min(under1))*m,np.max(upper1 + np.abs(np.max(upper1))*m)])\n",
    "axs[1,0].set_ylim([np.min(under2)-np.abs(np.min(under2))*m,np.max(upper2 + np.abs(np.max(upper2))*m)])\n",
    "axs[1,1].set_ylim([np.min(under3)-np.abs(np.min(under3))*m,np.max(upper3 + np.abs(np.max(upper3))*m)])\n",
    "axs[2,0].set_ylim([np.min(under4)-np.abs(np.min(under4))*m,np.max(upper4 + np.abs(np.max(upper4))*m)])\n",
    "axs[2,1].set_ylim([np.min(under5)-np.abs(np.min(under5))*m,np.max(upper5 + np.abs(np.max(upper5))*m)])\n",
    "axs[3,0].set_ylim([np.min(under6)-np.abs(np.min(under6))*m,np.max(upper6 + np.abs(np.max(upper6))*m)])\n",
    "axs[3,1].set_ylim([np.min(under7)-np.abs(np.min(under7))*m,np.max(upper7 + np.abs(np.max(upper7))*m)])\n",
    "#   \n",
    "\n",
    "# set the title to subplots\n",
    "axs[0, 0].set_title('intercept').set_fontsize(ft)\n",
    "axs[0, 1].set_title(column_names[0]).set_fontsize(ft)\n",
    "axs[1, 0].set_title(column_names[1]).set_fontsize(ft)\n",
    "axs[1, 1].set_title(column_names[2]).set_fontsize(ft)\n",
    "axs[2, 0].set_title(column_names[3]).set_fontsize(ft)\n",
    "axs[2, 1].set_title(column_names[4]).set_fontsize(ft)\n",
    "axs[3, 0].set_title(column_names[5]).set_fontsize(ft)\n",
    "axs[3, 1].set_title(column_names[6]).set_fontsize(ft)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# set spacing\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.savefig('logistic_regression2', dpi = 300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d9ce19-d610-49c0-9ad3-b4b548e0364c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "opmerking = hier is een zekere drifting te zien van de parameters\n",
    "het omslagpunt vanaf  meting 7000 valt op."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92291391-cd13-40b2-9023-a04ce01faeff",
   "metadata": {
    "heading_collapsed": true,
    "tags": []
   },
   "source": [
    "## CHECKING THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee98caa3-fc85-4a8a-b2d8-a3f758a01b92",
   "metadata": {
    "hidden": true,
    "tags": []
   },
   "source": [
    "### Logistic model input parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9a2853-155a-496a-9171-ed2fd10fdb4b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "P(y=1|y) = 1/1+e-(b0 + b1x1 + .... + bnXn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba274fb6",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "betameans_set = SUMMARY[i][2]\n",
    "n = 4\n",
    "varfeature = [sublist[n] for sublist in X]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184ed548",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "param = 0.5\n",
    "\n",
    "a= 0\n",
    "i = 3\n",
    "betameans_set = SUMMARY[i][2]\n",
    "\n",
    "##TRUE VALUES\n",
    "Y_orig = [0 if y == -1 else 1 for y in Y][a:]\n",
    "\n",
    "## LOGISTIC PREDICTION \n",
    "\n",
    "regressionterm_components = np.array(betameans_set[a:])*X[a:]\n",
    "regressionterm = [np.sum(comp) for comp in regressionterm_components]\n",
    "Y_pred = [1 / (1 + np.exp(-rt)) for rt in regressionterm] \n",
    "Y_est = [1 if y > param else 0 for y in Y_pred]\n",
    "conf_mat = confusion_matrix(Y_orig, Y_est)\n",
    "\n",
    "TP , FP, FN , TN = conf_mat.ravel()\n",
    "Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "Precision = TP / (TP + FP) \n",
    "\n",
    "\n",
    "# create labels\n",
    "labels = ['INTRUSION', 'OK']\n",
    "\n",
    "# create heatmap with labels\n",
    "ax = sns.heatmap(conf_mat, annot=True, cmap='Blues', fmt='g', xticklabels=labels, yticklabels=labels)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True')\n",
    "\n",
    "ax.set_title('Confusion Matrix\\nAccuracy={:.2f}%, Precision={:.2f}%'.format(Accuracy*100, Precision*100))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd23162c",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = 0\n",
    "i = 3\n",
    "betameans_set = SUMMARY[i][2]\n",
    "\n",
    "# TRUE VALUES\n",
    "Y_orig = [0 if y == -1 else 1 for y in Y][a:]\n",
    "\n",
    "# create labels\n",
    "labels = ['INTRUSION', 'OK']\n",
    "\n",
    "# plot two heatmaps horizontally\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "for j, param in enumerate([0.5,  0.7699999999999997]):\n",
    "    # LOGISTIC PREDICTION\n",
    "    regressionterm_components = np.array(betameans_set[a:]) * X[a:]\n",
    "    regressionterm = [np.sum(comp) for comp in regressionterm_components]\n",
    "    Y_pred = [1 / (1 + np.exp(-rt)) for rt in regressionterm]\n",
    "    Y_est = [1 if y > param else 0 for y in Y_pred]\n",
    "    conf_mat = confusion_matrix(Y_orig, Y_est)\n",
    "\n",
    "    TP, FP, FN, TN = conf_mat.ravel()\n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Precision = TP / (TP + FP)\n",
    "\n",
    "    # create heatmap with labels\n",
    "    ax = sns.heatmap(conf_mat, annot=True, cmap='RdYlBu', fmt='g', xticklabels=labels, yticklabels=labels, ax=axs[j])\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('True')\n",
    "    ax.set_title('Treshold = {:.2f}\\nAccuracy = {:.2f}%, Precision = {:.2f}%'.format(param, Accuracy * 100, Precision * 100))\n",
    "fig.suptitle('Confusion Matrix with default and tuned treshold ', fontsize=16, y=1.01)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion logistic', dpi = 300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a51f80",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Select the nth column\n",
    "n = 4\n",
    "nth_column = column_names[n]\n",
    "print(\"Nth column:\", nth_column)\n",
    "\n",
    "# Create a vector with all columns except the nth\n",
    "other_columns = [col for i, col in enumerate(column_names) if i != n]\n",
    "print(\"Other columns:\", other_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231d0ea8",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "a = 0\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Define a range of parameter values to test\n",
    "param_range = np.arange(0.1, 1.0, 0.01)\n",
    "\n",
    "# Initialize variables to store the best parameter value and performance\n",
    "best_param = 0\n",
    "best_score = 0\n",
    "best_accuracy = 0\n",
    "\n",
    "# Loop through each parameter value and calculate the F1 score\n",
    "for param in param_range:\n",
    "    # Generate predictions based on the current parameter value\n",
    "    Y_est = [1 if y > param else 0 for y in Y_pred]\n",
    "    \n",
    "    # Calculate the confusion matrix\n",
    "    \n",
    "    \n",
    "    TP , FP, FN , TN = confusion_matrix(Y_orig[a:], Y_est[a:]).ravel()\n",
    "\n",
    "    Accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    Precision = TP / (TP + FP)  #correctly predicted divided by \n",
    "    \n",
    "    \n",
    "    # Calculate the F1 score based on the confusion matrix\n",
    "    f1 = f1_score(Y_orig, Y_est)\n",
    "    \n",
    "    # If the current F1 score is better than the previous best and true negatives are at least 80%,\n",
    "    # update the best values\n",
    "    if Accuracy > best_accuracy and Precision >= .50:\n",
    "        best_param = param\n",
    "        best_accuracy = Accuracy\n",
    "        best_precision = Precision\n",
    "\n",
    "# Print the best parameter value and F1 score\n",
    "print(f\"Best parameter value: {best_param}\")\n",
    "print(f\"precision: {best_precision}\")\n",
    "print(f\"Accuracy: {Accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc-autonumbering": true,
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
